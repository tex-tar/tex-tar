output_categories: [4,4]
sequence_size: 125
optimizer: adam
lr: 0.0001
model: consent_rope_selective
loss_fn:
  train: train_loss
  val: val_loss
label_split:
  - [4, 4]
loss_weights:
  - 0.25
  - 0.75
loss_types:
  train: [ce,ce]
  val: [ce,ce]
datasets:
  train: /data/textar_outputs/word_cw
  val: /data/textar_outputs/word_cw
batch_size: 1600
pretrained: "False"
load_pretrained_function: "False"
accumulated_batch_descent: 2
dataloader:
  train: DocLevelDataset_RoPE_Train
  val: DocLevelDataset_RoPE_Val_name
num_workers: 8
use_wandb: true
run_name: test-run-1
shuffle: true
wandb_project_name: T1-9Aug
epochs: 100